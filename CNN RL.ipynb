{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christos Christidis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use: tensorboard --logdir=runs to run the Tensorboard session and view the results if the images below don't work. Replace \"runs\" with your own directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "\"num_episodes\" :  1000, #1,  # Change to 1000, 2000\n",
    "\"batch_size\" : 32,\n",
    "\"gamma\" : 0.9, #0.99, # Discount factor\n",
    "\"epsilon_start\" : 0.4, # 1.0, # Try smaller epsilon start for less exploration\n",
    "\"epsilon_end\" : 0.01,\n",
    "\"epsilon_decay\" : 0.995, # Not used\n",
    "\"target_update\" : 10,\n",
    "\"device\" :  torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "\"learning_rate\" : 0.003 #0.005\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \"gym[atari, accept-rom-license]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=1, stride=1)\n",
    "        self.fc1 = nn.Linear(1280, 512) #(3136, 512) # Based on RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1280 and 3136x512), we change from 3136 to 1280\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make('Assault-v4') # render_mode='human' <-- Crashes!!! # Try with a different game too, it's plug and play! v4 has no repeated action enabled\n",
    "#env = gym.make('Centipede-v4') \n",
    "input_channels = env.observation_space.shape[0] # Input for the cnns should be states\n",
    "output_dim = env.action_space.n # Output of the cnns should be an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9 available actions\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observation space: (lower bound, upper bound, shape, dtype)\n",
    "env.observation_space#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNPolicy(\n",
       "  (conv1): Conv2d(210, 32, kernel_size=(3, 3), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize policy and target networks\n",
    "policy_net = CNNPolicy(input_channels, output_dim).to(parameters[\"device\"])\n",
    "target_net = CNNPolicy(input_channels, output_dim).to(parameters[\"device\"])\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=parameters['learning_rate'])\n",
    "replay_buffer = deque(maxlen=32)\n",
    "epsilon = parameters['epsilon_start']\n",
    "epsilon_decay_step = (parameters['epsilon_start'] - parameters['epsilon_end']) / parameters['num_episodes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://openreview.net/pdf/75f0008512b0ab359f0fdbba5551d26760b7bce8.pdf, during the training phase should be used at most 200M frames and end the episode when all lives are lost or the episode exceeds 30mins. Also it is suggested that the agents are trained on at least 10m steps. Due to resource limitations such great lengths of episodes were not met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "steps = 0\n",
    "\n",
    "for episode in range(parameters['num_episodes']):\n",
    "    state = env.reset(seed=42)[0] # env.reset() returns observation state + info so we only keep the state\n",
    "    #print(state)\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "    policy_loss_sum = 0\n",
    "    value_loss_sum = 0\n",
    "    done = False\n",
    "    fault = False\n",
    "\n",
    "    while not done:\n",
    "        episode_length += 1\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() # Select a random action to do\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(parameters['device'])\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = q_values.argmax().item()  # Select the best action\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)  # Returns 5: sample of obs_space (next state), reward, terminated we need and truncated, info which we don't need\n",
    "        total_reward += reward\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # Perform gradient descent step\n",
    "        if len(replay_buffer) > parameters['batch_size']:\n",
    "            print(\"replay_buffer > 32\")\n",
    "            batch = random.sample(replay_buffer, parameters['batch_size'])\n",
    "        else:\n",
    "            batch = replay_buffer\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Sometimes the state is returned either as a tuple or as an ndarray. The issue rises when it's returned as a tuple.\n",
    "        has_tuple = any(isinstance(element, tuple) for element in states)\n",
    "        if has_tuple:\n",
    "            continue # Simply skip the loop that contains faulty data.\n",
    "        \n",
    "\n",
    "        # Creating tensors but need to be converted into ndarrays first for faster loading.\n",
    "        states = torch.from_numpy(np.array(states)).to(parameters['device'])/255 # Normalizing the tensor which will also convert it into float32 which the model accepts\n",
    "        actions = torch.from_numpy(np.array(actions)).to(parameters['device'])\n",
    "        rewards = torch.from_numpy(np.array(rewards)).to(parameters['device']).float()\n",
    "        next_states = torch.from_numpy(np.array(next_states)).to(parameters['device'])/255\n",
    "        dones = torch.from_numpy(np.array(dones)).to(parameters['device'])\n",
    "\n",
    "        q_values = policy_net(states).gather(1, actions.type(torch.int64).unsqueeze(1)).squeeze(1) # Changing the data type of actions tensor before passing it to torch.gather\n",
    "        next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (~dones) * parameters['gamma'] * next_q_values\n",
    "        \n",
    "        loss = F.mse_loss(q_values, target_q_values) \n",
    "        loss = loss.type(torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps+=1 \n",
    "        \n",
    "        policy_loss_sum += loss.item()\n",
    "        value_loss_sum += loss.item()\n",
    "\n",
    "    \n",
    "    \n",
    "    if not fault:\n",
    "        # Decay epsilon\n",
    "        epsilon = max(parameters['epsilon_end'], epsilon - epsilon_decay_step)\n",
    "\n",
    "        # Update target network periodically\n",
    "        if episode % parameters['target_update'] == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Logging on TensorBoard\n",
    "        writer.add_scalar('Training/Cumulative_Reward', total_reward, episode)\n",
    "        writer.add_scalar('Training/Episode_Length', episode_length, episode)\n",
    "        writer.add_scalar('Training/Policy_Loss', policy_loss_sum / episode_length, episode)\n",
    "        writer.add_scalar('Training/Value_Loss', value_loss_sum / episode_length, episode)\n",
    "\n",
    "        # Print episode information\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}, Episode Length: {episode_length}')\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assault - Explore - Total number of steps in 1000 episodes: 507143\n",
    "* Assault - Explore - Total number of steps in 2000 episodes: 980745\n",
    "* Centipede - Explore - Total number of steps in 1000 episodes: 714922\n",
    "\n",
    "* Assault - Force Exploitation - Total number of steps in 1000 episodes: 849296\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps:  849296\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of steps: \", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename=\"./Logs/first test run 100 episodes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image(filename=\"./Logs/1000 ep run.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "file_path = '1k_assault_forced_exploit_policy_cnn.pth'\n",
    "torch.save(policy_net.state_dict(), file_path)\n",
    "file_path = '1k_assault_forced_exploit__target_cnn.pth'\n",
    "torch.save(target_net.state_dict(), file_path)\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy CNN loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = './trained states' # Might need to edit depending on where you save.\n",
    "file_name = \"1k_assault_policy_cnn.pth\" #'1k_assault_policy_cnn.pth'\n",
    "pth_file_path = os.path.join(path, file_name)\n",
    "pre_trained_policy_cnn = CNNPolicy(input_channels, output_dim) # Need to make the environment first to get the input and output dim <- Run Model and Environment Initialization\n",
    "pre_trained_policy_cnn.load_state_dict(torch.load(pth_file_path))\n",
    "pre_trained_policy_cnn.to(parameters['device']) # Need to run settings first to get the device <- Check section \"Settings\"\n",
    "print(\"Policy CNN loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_policy_cnn.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the trained agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to take a look on how the environment looks, that's the way to do it below. You can include it in the training loop but it might slow the training process, especially for a large number of episodes. \n",
    "* Disclaimer: It might cause the kernel to crash if you use VSCode once it's finished or you manually close it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "pre_trained_policy_cnn.eval()\n",
    "env = gym.make(\"Assault-v4\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "state_tensor = torch.FloatTensor(observation).unsqueeze(0).to(parameters['device'])\n",
    "q_values = policy_net(state_tensor)\n",
    "action = q_values.argmax().item()  # Select the best action\n",
    "\n",
    "for _ in range(2000):\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
