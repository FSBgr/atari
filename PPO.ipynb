{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"total_time_steps\" : 1000000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'AssaultNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment to stack frames and normalize observations\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"./ppo_assault_tensorboard/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model\n",
    "model = PPO('CnnPolicy', env, verbose=0, tensorboard_log=tensorboard_log_dir) # Change verbose to 1 for info messages and 2 for debug messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for evaluation and saving models\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./logs/', name_prefix='ppo_assault') # Save checkpoint trained state every 10k time steps. Might need to remove\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model/',\n",
    "                             log_path='./logs/results', eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x0000021C295A3B10> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x0000021C295CE590>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=231.00 +/- 35.14\n",
      "Episode length: 3477.60 +/- 403.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=344.40 +/- 156.92\n",
      "Episode length: 3351.20 +/- 1317.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=432.60 +/- 54.11\n",
      "Episode length: 3887.20 +/- 536.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=449.40 +/- 89.69\n",
      "Episode length: 3509.40 +/- 681.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=483.00 +/- 176.20\n",
      "Episode length: 3147.00 +/- 811.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=491.40 +/- 87.70\n",
      "Episode length: 3119.00 +/- 582.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=424.20 +/- 135.05\n",
      "Episode length: 2546.00 +/- 671.59\n",
      "Eval num_timesteps=80000, episode_reward=386.40 +/- 94.48\n",
      "Episode length: 2836.00 +/- 685.39\n",
      "Eval num_timesteps=90000, episode_reward=319.20 +/- 97.96\n",
      "Episode length: 2580.20 +/- 620.48\n",
      "Eval num_timesteps=100000, episode_reward=520.80 +/- 123.45\n",
      "Episode length: 2940.20 +/- 831.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=411.60 +/- 145.86\n",
      "Episode length: 2573.20 +/- 646.76\n",
      "Eval num_timesteps=120000, episode_reward=411.60 +/- 166.74\n",
      "Episode length: 2590.00 +/- 836.72\n",
      "Eval num_timesteps=130000, episode_reward=462.00 +/- 85.04\n",
      "Episode length: 2816.60 +/- 570.59\n",
      "Eval num_timesteps=140000, episode_reward=457.80 +/- 104.07\n",
      "Episode length: 3260.60 +/- 607.52\n",
      "Eval num_timesteps=150000, episode_reward=692.60 +/- 85.33\n",
      "Episode length: 3862.60 +/- 436.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=487.20 +/- 86.48\n",
      "Episode length: 2850.00 +/- 422.73\n",
      "Eval num_timesteps=170000, episode_reward=445.20 +/- 82.30\n",
      "Episode length: 3181.60 +/- 476.52\n",
      "Eval num_timesteps=180000, episode_reward=415.80 +/- 94.29\n",
      "Episode length: 2716.00 +/- 563.58\n",
      "Eval num_timesteps=190000, episode_reward=457.80 +/- 131.75\n",
      "Episode length: 3087.20 +/- 564.42\n",
      "Eval num_timesteps=200000, episode_reward=411.60 +/- 31.43\n",
      "Episode length: 2926.80 +/- 469.47\n",
      "Eval num_timesteps=210000, episode_reward=407.40 +/- 63.14\n",
      "Episode length: 2849.00 +/- 281.98\n",
      "Eval num_timesteps=220000, episode_reward=407.40 +/- 101.67\n",
      "Episode length: 3037.40 +/- 652.15\n",
      "Eval num_timesteps=230000, episode_reward=407.40 +/- 34.12\n",
      "Episode length: 2693.40 +/- 366.47\n",
      "Eval num_timesteps=240000, episode_reward=432.60 +/- 79.25\n",
      "Episode length: 2760.20 +/- 360.81\n",
      "Eval num_timesteps=250000, episode_reward=319.20 +/- 74.42\n",
      "Episode length: 2362.60 +/- 633.41\n",
      "Eval num_timesteps=260000, episode_reward=327.60 +/- 94.48\n",
      "Episode length: 2379.80 +/- 522.03\n",
      "Eval num_timesteps=270000, episode_reward=424.20 +/- 90.47\n",
      "Episode length: 2932.20 +/- 607.57\n",
      "Eval num_timesteps=280000, episode_reward=436.80 +/- 48.62\n",
      "Episode length: 2994.40 +/- 362.91\n",
      "Eval num_timesteps=290000, episode_reward=420.00 +/- 102.88\n",
      "Episode length: 3301.40 +/- 587.86\n",
      "Eval num_timesteps=300000, episode_reward=369.60 +/- 50.75\n",
      "Episode length: 2481.00 +/- 406.54\n",
      "Eval num_timesteps=310000, episode_reward=508.20 +/- 79.02\n",
      "Episode length: 3052.00 +/- 500.37\n",
      "Eval num_timesteps=320000, episode_reward=491.40 +/- 43.24\n",
      "Episode length: 3019.60 +/- 292.74\n",
      "Eval num_timesteps=330000, episode_reward=525.00 +/- 118.05\n",
      "Episode length: 3168.60 +/- 354.80\n",
      "Eval num_timesteps=340000, episode_reward=548.00 +/- 115.31\n",
      "Episode length: 3237.40 +/- 990.86\n",
      "Eval num_timesteps=350000, episode_reward=411.60 +/- 133.87\n",
      "Episode length: 2725.80 +/- 616.73\n",
      "Eval num_timesteps=360000, episode_reward=522.80 +/- 118.31\n",
      "Episode length: 3100.00 +/- 656.30\n",
      "Eval num_timesteps=370000, episode_reward=512.40 +/- 107.57\n",
      "Episode length: 3202.40 +/- 518.58\n",
      "Eval num_timesteps=380000, episode_reward=369.60 +/- 91.63\n",
      "Episode length: 2700.00 +/- 578.28\n",
      "Eval num_timesteps=390000, episode_reward=520.80 +/- 93.35\n",
      "Episode length: 3125.60 +/- 502.56\n",
      "Eval num_timesteps=400000, episode_reward=537.60 +/- 88.70\n",
      "Episode length: 2837.60 +/- 511.61\n",
      "Eval num_timesteps=410000, episode_reward=564.80 +/- 162.62\n",
      "Episode length: 3170.60 +/- 1027.66\n",
      "Eval num_timesteps=420000, episode_reward=581.60 +/- 66.12\n",
      "Episode length: 3262.40 +/- 274.56\n",
      "Eval num_timesteps=430000, episode_reward=491.40 +/- 188.11\n",
      "Episode length: 2951.00 +/- 561.12\n",
      "Eval num_timesteps=440000, episode_reward=432.60 +/- 190.44\n",
      "Episode length: 3192.60 +/- 1530.09\n",
      "Eval num_timesteps=450000, episode_reward=432.60 +/- 142.18\n",
      "Episode length: 2597.80 +/- 494.23\n",
      "Eval num_timesteps=460000, episode_reward=478.80 +/- 95.22\n",
      "Episode length: 3042.20 +/- 336.87\n",
      "Eval num_timesteps=470000, episode_reward=424.20 +/- 103.22\n",
      "Episode length: 2874.60 +/- 528.59\n",
      "Eval num_timesteps=480000, episode_reward=466.20 +/- 138.92\n",
      "Episode length: 3402.60 +/- 738.06\n",
      "Eval num_timesteps=490000, episode_reward=508.20 +/- 177.40\n",
      "Episode length: 3691.80 +/- 1012.18\n",
      "Eval num_timesteps=500000, episode_reward=504.00 +/- 63.70\n",
      "Episode length: 3445.40 +/- 315.33\n",
      "Eval num_timesteps=510000, episode_reward=588.00 +/- 51.44\n",
      "Episode length: 3818.60 +/- 349.96\n",
      "Eval num_timesteps=520000, episode_reward=466.20 +/- 112.23\n",
      "Episode length: 2839.20 +/- 671.15\n",
      "Eval num_timesteps=530000, episode_reward=632.00 +/- 80.78\n",
      "Episode length: 4204.20 +/- 674.03\n",
      "Eval num_timesteps=540000, episode_reward=369.60 +/- 137.77\n",
      "Episode length: 2824.60 +/- 1076.28\n",
      "Eval num_timesteps=550000, episode_reward=508.20 +/- 93.35\n",
      "Episode length: 3601.20 +/- 597.72\n",
      "Eval num_timesteps=560000, episode_reward=590.00 +/- 112.21\n",
      "Episode length: 3724.40 +/- 565.61\n",
      "Eval num_timesteps=570000, episode_reward=365.40 +/- 90.67\n",
      "Episode length: 2586.40 +/- 403.18\n",
      "Eval num_timesteps=580000, episode_reward=441.00 +/- 134.79\n",
      "Episode length: 2731.80 +/- 774.85\n",
      "Eval num_timesteps=590000, episode_reward=533.40 +/- 119.98\n",
      "Episode length: 3581.20 +/- 476.47\n",
      "Eval num_timesteps=600000, episode_reward=499.80 +/- 113.79\n",
      "Episode length: 3380.60 +/- 476.61\n",
      "Eval num_timesteps=610000, episode_reward=556.40 +/- 172.31\n",
      "Episode length: 4208.40 +/- 1237.95\n",
      "Eval num_timesteps=620000, episode_reward=327.60 +/- 123.60\n",
      "Episode length: 2199.20 +/- 593.37\n",
      "Eval num_timesteps=630000, episode_reward=508.20 +/- 171.84\n",
      "Episode length: 3890.00 +/- 400.43\n",
      "Eval num_timesteps=640000, episode_reward=556.20 +/- 107.49\n",
      "Episode length: 3845.40 +/- 882.50\n",
      "Eval num_timesteps=650000, episode_reward=449.40 +/- 182.40\n",
      "Episode length: 3068.40 +/- 1360.27\n",
      "Eval num_timesteps=660000, episode_reward=512.40 +/- 160.81\n",
      "Episode length: 4318.20 +/- 1382.95\n",
      "Eval num_timesteps=670000, episode_reward=394.80 +/- 95.22\n",
      "Episode length: 3790.60 +/- 1008.76\n",
      "Eval num_timesteps=680000, episode_reward=527.00 +/- 133.79\n",
      "Episode length: 4282.60 +/- 1017.51\n",
      "Eval num_timesteps=690000, episode_reward=504.00 +/- 153.17\n",
      "Episode length: 4849.20 +/- 1744.99\n",
      "Eval num_timesteps=700000, episode_reward=562.60 +/- 133.52\n",
      "Episode length: 6333.80 +/- 1240.96\n",
      "Eval num_timesteps=710000, episode_reward=583.80 +/- 73.23\n",
      "Episode length: 4132.60 +/- 598.23\n",
      "Eval num_timesteps=720000, episode_reward=501.80 +/- 229.19\n",
      "Episode length: 3255.60 +/- 1168.55\n",
      "Eval num_timesteps=730000, episode_reward=579.60 +/- 71.03\n",
      "Episode length: 3955.40 +/- 642.64\n",
      "Eval num_timesteps=740000, episode_reward=615.20 +/- 33.46\n",
      "Episode length: 5596.20 +/- 1179.86\n",
      "Eval num_timesteps=750000, episode_reward=663.40 +/- 40.42\n",
      "Episode length: 5400.80 +/- 831.06\n",
      "Eval num_timesteps=760000, episode_reward=487.20 +/- 103.22\n",
      "Episode length: 3793.20 +/- 819.45\n",
      "Eval num_timesteps=770000, episode_reward=602.40 +/- 135.62\n",
      "Episode length: 4314.00 +/- 1275.69\n",
      "Eval num_timesteps=780000, episode_reward=589.60 +/- 182.53\n",
      "Episode length: 4625.00 +/- 1009.07\n",
      "Eval num_timesteps=790000, episode_reward=541.80 +/- 61.44\n",
      "Episode length: 4293.60 +/- 634.02\n",
      "Eval num_timesteps=800000, episode_reward=617.20 +/- 97.74\n",
      "Episode length: 4473.20 +/- 421.71\n",
      "Eval num_timesteps=810000, episode_reward=659.20 +/- 100.83\n",
      "Episode length: 4907.00 +/- 1056.68\n",
      "Eval num_timesteps=820000, episode_reward=602.60 +/- 83.77\n",
      "Episode length: 4680.80 +/- 806.13\n",
      "Eval num_timesteps=830000, episode_reward=510.20 +/- 92.26\n",
      "Episode length: 4531.00 +/- 1385.52\n",
      "Eval num_timesteps=840000, episode_reward=627.60 +/- 125.54\n",
      "Episode length: 4936.00 +/- 538.90\n",
      "Eval num_timesteps=850000, episode_reward=394.80 +/- 146.34\n",
      "Episode length: 3003.00 +/- 1025.82\n",
      "Eval num_timesteps=860000, episode_reward=569.00 +/- 118.40\n",
      "Episode length: 4502.60 +/- 1162.62\n",
      "Eval num_timesteps=870000, episode_reward=512.40 +/- 171.94\n",
      "Episode length: 3947.00 +/- 1266.75\n",
      "Eval num_timesteps=880000, episode_reward=340.20 +/- 52.12\n",
      "Episode length: 2759.00 +/- 427.29\n",
      "Eval num_timesteps=890000, episode_reward=462.00 +/- 76.30\n",
      "Episode length: 3365.80 +/- 612.73\n",
      "Eval num_timesteps=900000, episode_reward=516.60 +/- 88.70\n",
      "Episode length: 3428.00 +/- 498.65\n",
      "Eval num_timesteps=910000, episode_reward=527.00 +/- 141.06\n",
      "Episode length: 3499.80 +/- 807.55\n",
      "Eval num_timesteps=920000, episode_reward=453.60 +/- 110.00\n",
      "Episode length: 3407.20 +/- 611.51\n",
      "Eval num_timesteps=930000, episode_reward=428.40 +/- 79.25\n",
      "Episode length: 3315.80 +/- 678.53\n",
      "Eval num_timesteps=940000, episode_reward=550.20 +/- 107.41\n",
      "Episode length: 3944.80 +/- 541.91\n",
      "Eval num_timesteps=950000, episode_reward=390.60 +/- 122.88\n",
      "Episode length: 2904.60 +/- 957.78\n",
      "Eval num_timesteps=960000, episode_reward=466.20 +/- 142.68\n",
      "Episode length: 3698.20 +/- 1148.03\n",
      "Eval num_timesteps=970000, episode_reward=562.80 +/- 123.45\n",
      "Episode length: 3925.80 +/- 893.86\n",
      "Eval num_timesteps=980000, episode_reward=558.60 +/- 85.66\n",
      "Episode length: 4122.00 +/- 545.44\n",
      "Eval num_timesteps=990000, episode_reward=567.00 +/- 65.07\n",
      "Episode length: 4655.80 +/- 676.86\n",
      "Eval num_timesteps=1000000, episode_reward=630.00 +/- 91.05\n",
      "Episode length: 4312.80 +/- 653.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21c29632650>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=parameters['total_time_steps'], callback=[checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time Elapsed 1m steps: 120m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_assault_1m_first_run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: Load policy model and run for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
