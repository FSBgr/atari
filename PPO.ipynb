{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.utils import get_linear_fn\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"total_time_steps\" : 1000000,\n",
    "    \"checkpoint_freq\" : 200000,\n",
    "    \"eval_freq\" : 50000,\n",
    "    \"n_steps\" : 2048,\n",
    "    \"batch_size\" : 64,\n",
    "    \"gae_lambda\" : 0.95,\n",
    "    \"ent_coef\" : 0.01,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"verbose\" : 0,\n",
    "    \"clip_range\" : 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'AssaultNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment to stack frames and normalize observations\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"./ppo_assault_tensorboard/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model\n",
    "#model = PPO('CnnPolicy', env, verbose=0, tensorboard_log=tensorboard_log_dir) # Change verbose to 1 for info messages and 2 for debug messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned model with custom actor-critic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCnnPolicy(ActorCriticCnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs,\n",
    "            net_arch=[dict(pi=[256, 256], vf=[256, 256])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learning_rate_schedule = get_linear_fn(start=3e-4, end=1e-6, end_fraction=0.9)\n",
    "model = PPO(CustomCnnPolicy, env, learning_rate=learning_rate_schedule, verbose=parameters['verbose'], \n",
    "            tensorboard_log=tensorboard_log_dir, n_steps=parameters['n_steps'], \n",
    "            batch_size=parameters['batch_size'], clip_range=parameters['clip_range'], gae_lambda=parameters['gae_lambda'], \n",
    "            ent_coef=parameters['ent_coef'], gamma=parameters['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for evaluation and saving models\n",
    "#checkpoint_callback = CheckpointCallback(save_freq=parameters['checkpoint_freq'], save_path='./logs/', name_prefix='ppo_assault_2m') # Save checkpoint trained state every 10k time steps. Might need to remove\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model/assault_1m_steps_tuned',\n",
    "                             log_path='./logs/results', eval_freq=parameters['eval_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000002520CCCEED0> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x000002520CCCF0D0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=50000, episode_reward=302.40 +/- 43.24\n",
      "Episode length: 2583.20 +/- 579.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=449.40 +/- 60.28\n",
      "Episode length: 3048.40 +/- 395.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=390.60 +/- 16.80\n",
      "Episode length: 3124.00 +/- 359.69\n",
      "Eval num_timesteps=200000, episode_reward=260.40 +/- 99.92\n",
      "Episode length: 2900.00 +/- 1006.90\n",
      "Eval num_timesteps=250000, episode_reward=260.40 +/- 54.11\n",
      "Episode length: 2565.20 +/- 321.76\n",
      "Eval num_timesteps=300000, episode_reward=348.60 +/- 148.85\n",
      "Episode length: 2766.60 +/- 559.65\n",
      "Eval num_timesteps=350000, episode_reward=411.60 +/- 75.83\n",
      "Episode length: 3026.20 +/- 453.08\n",
      "Eval num_timesteps=400000, episode_reward=568.80 +/- 93.18\n",
      "Episode length: 4570.80 +/- 1031.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=415.60 +/- 197.79\n",
      "Episode length: 3401.20 +/- 1213.71\n",
      "Eval num_timesteps=500000, episode_reward=525.00 +/- 75.13\n",
      "Episode length: 3455.00 +/- 560.80\n",
      "Eval num_timesteps=550000, episode_reward=457.60 +/- 153.64\n",
      "Episode length: 3277.60 +/- 904.44\n",
      "Eval num_timesteps=600000, episode_reward=504.00 +/- 96.69\n",
      "Episode length: 3422.80 +/- 822.97\n",
      "Eval num_timesteps=650000, episode_reward=550.20 +/- 141.43\n",
      "Episode length: 4163.20 +/- 492.24\n",
      "Eval num_timesteps=700000, episode_reward=571.20 +/- 101.50\n",
      "Episode length: 4178.00 +/- 805.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=750000, episode_reward=495.60 +/- 80.35\n",
      "Episode length: 4025.00 +/- 568.33\n",
      "Eval num_timesteps=800000, episode_reward=627.80 +/- 67.06\n",
      "Episode length: 5171.00 +/- 1185.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=850000, episode_reward=548.00 +/- 132.72\n",
      "Episode length: 3825.20 +/- 634.38\n",
      "Eval num_timesteps=900000, episode_reward=718.00 +/- 46.43\n",
      "Episode length: 5348.60 +/- 373.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=950000, episode_reward=558.40 +/- 104.83\n",
      "Episode length: 4235.20 +/- 872.88\n",
      "Eval num_timesteps=1000000, episode_reward=623.60 +/- 65.55\n",
      "Episode length: 4304.60 +/- 281.54\n",
      "Eval num_timesteps=1050000, episode_reward=516.40 +/- 117.49\n",
      "Episode length: 3991.40 +/- 1005.41\n",
      "Eval num_timesteps=1100000, episode_reward=520.80 +/- 103.22\n",
      "Episode length: 3694.80 +/- 618.44\n",
      "Eval num_timesteps=1150000, episode_reward=512.40 +/- 125.72\n",
      "Episode length: 3606.80 +/- 891.96\n",
      "Eval num_timesteps=1200000, episode_reward=436.80 +/- 89.49\n",
      "Episode length: 3696.20 +/- 707.77\n",
      "Eval num_timesteps=1250000, episode_reward=648.60 +/- 40.42\n",
      "Episode length: 4542.20 +/- 595.56\n",
      "Eval num_timesteps=1300000, episode_reward=390.60 +/- 111.60\n",
      "Episode length: 3258.20 +/- 734.70\n",
      "Eval num_timesteps=1350000, episode_reward=445.20 +/- 105.75\n",
      "Episode length: 3914.40 +/- 689.88\n",
      "Eval num_timesteps=1400000, episode_reward=464.00 +/- 138.90\n",
      "Episode length: 4026.20 +/- 959.41\n",
      "Eval num_timesteps=1450000, episode_reward=455.60 +/- 145.28\n",
      "Episode length: 3664.20 +/- 996.19\n",
      "Eval num_timesteps=1500000, episode_reward=470.40 +/- 93.54\n",
      "Episode length: 3789.20 +/- 891.44\n",
      "Eval num_timesteps=1550000, episode_reward=361.20 +/- 131.08\n",
      "Episode length: 3130.60 +/- 763.59\n",
      "Eval num_timesteps=1600000, episode_reward=640.40 +/- 64.19\n",
      "Episode length: 4574.60 +/- 669.76\n",
      "Eval num_timesteps=1650000, episode_reward=533.20 +/- 192.40\n",
      "Episode length: 3984.00 +/- 1109.40\n",
      "Eval num_timesteps=1700000, episode_reward=478.80 +/- 131.75\n",
      "Episode length: 3755.60 +/- 726.67\n",
      "Eval num_timesteps=1750000, episode_reward=424.20 +/- 85.46\n",
      "Episode length: 3005.60 +/- 536.22\n",
      "Eval num_timesteps=1800000, episode_reward=462.00 +/- 75.13\n",
      "Episode length: 4088.80 +/- 1223.35\n",
      "Eval num_timesteps=1850000, episode_reward=485.00 +/- 124.15\n",
      "Episode length: 3485.40 +/- 406.68\n",
      "Eval num_timesteps=1900000, episode_reward=499.40 +/- 152.15\n",
      "Episode length: 3647.00 +/- 962.72\n",
      "Eval num_timesteps=1950000, episode_reward=189.00 +/- 189.70\n",
      "Episode length: 11052.60 +/- 13452.31\n",
      "Eval num_timesteps=2000000, episode_reward=37.80 +/- 20.58\n",
      "Episode length: 15171.00 +/- 7697.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x252141a4a50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.learn(total_timesteps=parameters['total_time_steps'], callback=[eval_callback]) #[checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time Elapsed 1m steps: 84m\n",
    "* Time Elapsed 2m steps: 140m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_assault_1m_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: Load policy model and run for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
