{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christos Christidis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.utils import get_linear_fn\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"total_time_steps\" : 1000000,#5000000,\n",
    "    \"checkpoint_freq\" : 200000,\n",
    "    \"eval_freq\" : 50000,\n",
    "    \"n_steps\" : 2048,\n",
    "    \"batch_size\" : 64,\n",
    "    \"gae_lambda\" : 0.95,\n",
    "    \"ent_coef\" : 0.01,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"verbose\" : 0,\n",
    "    \"clip_range\" : 0.2,\n",
    "    \"features_dim\" : 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'AssaultNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment to stack frames and normalize observations\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"./ppo_assault_tensorboard/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model\n",
    "#model = PPO('CnnPolicy', env, verbose=0, tensorboard_log=tensorboard_log_dir) # Change verbose to 1 for info messages and 2 for debug messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned model with custom actor-critic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCnnPolicy(ActorCriticCnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs,\n",
    "            net_arch=[dict(pi=[256, 256], vf=[256, 256])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_schedule = get_linear_fn(start=3e-4, end=1e-6, end_fraction=0.9)\n",
    "model = PPO(CustomCnnPolicy, env, learning_rate=learning_rate_schedule, verbose=parameters['verbose'], \n",
    "            tensorboard_log=tensorboard_log_dir, n_steps=parameters['n_steps'], \n",
    "            batch_size=parameters['batch_size'], clip_range=parameters['clip_range'], gae_lambda=parameters['gae_lambda'], \n",
    "            ent_coef=parameters['ent_coef'], gamma=parameters['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for evaluation and saving models\n",
    "#checkpoint_callback = CheckpointCallback(save_freq=parameters['checkpoint_freq'], save_path='./logs/', name_prefix='ppo_assault_2m') # Save checkpoint trained state every 10k time steps. Might need to remove\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model/assault_5m_steps_tuned',\n",
    "                             log_path='./logs/results', eval_freq=parameters['eval_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x0000018C5070CA50> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x0000018C4FF09A10>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=50000, episode_reward=323.40 +/- 54.11\n",
      "Episode length: 2200.80 +/- 388.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=323.40 +/- 36.61\n",
      "Episode length: 2332.00 +/- 265.96\n",
      "Eval num_timesteps=150000, episode_reward=369.60 +/- 97.24\n",
      "Episode length: 2495.00 +/- 446.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=399.00 +/- 116.55\n",
      "Episode length: 2645.80 +/- 450.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=250000, episode_reward=373.80 +/- 165.03\n",
      "Episode length: 2789.40 +/- 660.19\n",
      "Eval num_timesteps=300000, episode_reward=424.20 +/- 62.86\n",
      "Episode length: 3077.80 +/- 295.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=428.40 +/- 75.83\n",
      "Episode length: 2983.60 +/- 575.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=336.00 +/- 18.78\n",
      "Episode length: 2357.20 +/- 221.34\n",
      "Eval num_timesteps=450000, episode_reward=453.60 +/- 110.00\n",
      "Episode length: 3189.40 +/- 529.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500000, episode_reward=277.20 +/- 36.13\n",
      "Episode length: 2471.20 +/- 326.30\n",
      "Eval num_timesteps=550000, episode_reward=218.40 +/- 99.92\n",
      "Episode length: 1882.60 +/- 499.46\n",
      "Eval num_timesteps=600000, episode_reward=331.80 +/- 81.22\n",
      "Episode length: 2375.00 +/- 349.92\n",
      "Eval num_timesteps=650000, episode_reward=273.00 +/- 69.01\n",
      "Episode length: 2311.20 +/- 314.95\n",
      "Eval num_timesteps=700000, episode_reward=369.60 +/- 122.88\n",
      "Episode length: 3598.80 +/- 1064.75\n",
      "Eval num_timesteps=750000, episode_reward=581.60 +/- 104.44\n",
      "Episode length: 5213.00 +/- 726.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=518.60 +/- 86.29\n",
      "Episode length: 4873.40 +/- 853.17\n",
      "Eval num_timesteps=850000, episode_reward=430.40 +/- 199.78\n",
      "Episode length: 8024.40 +/- 2814.77\n",
      "Eval num_timesteps=900000, episode_reward=478.80 +/- 90.47\n",
      "Episode length: 6099.40 +/- 1773.74\n",
      "Eval num_timesteps=950000, episode_reward=411.60 +/- 113.94\n",
      "Episode length: 4466.40 +/- 1256.39\n",
      "Eval num_timesteps=1000000, episode_reward=398.80 +/- 184.19\n",
      "Episode length: 3218.00 +/- 1327.52\n",
      "Eval num_timesteps=1050000, episode_reward=394.80 +/- 156.25\n",
      "Episode length: 3294.80 +/- 1112.43\n",
      "Eval num_timesteps=1100000, episode_reward=394.80 +/- 36.13\n",
      "Episode length: 3205.60 +/- 367.58\n",
      "Eval num_timesteps=1150000, episode_reward=247.80 +/- 91.44\n",
      "Episode length: 1875.80 +/- 464.18\n",
      "Eval num_timesteps=1200000, episode_reward=319.20 +/- 102.36\n",
      "Episode length: 2317.60 +/- 572.97\n",
      "Eval num_timesteps=1250000, episode_reward=302.40 +/- 78.12\n",
      "Episode length: 2489.00 +/- 414.77\n",
      "Eval num_timesteps=1300000, episode_reward=365.40 +/- 50.75\n",
      "Episode length: 3200.80 +/- 323.09\n",
      "Eval num_timesteps=1350000, episode_reward=373.80 +/- 69.52\n",
      "Episode length: 2828.00 +/- 394.91\n",
      "Eval num_timesteps=1400000, episode_reward=487.20 +/- 75.60\n",
      "Episode length: 4533.20 +/- 428.72\n",
      "Eval num_timesteps=1450000, episode_reward=459.80 +/- 107.79\n",
      "Episode length: 4272.00 +/- 887.24\n",
      "Eval num_timesteps=1500000, episode_reward=415.80 +/- 70.78\n",
      "Episode length: 3387.80 +/- 642.13\n",
      "Eval num_timesteps=1550000, episode_reward=491.40 +/- 75.83\n",
      "Episode length: 3461.40 +/- 321.56\n",
      "Eval num_timesteps=1600000, episode_reward=386.40 +/- 72.26\n",
      "Episode length: 3201.00 +/- 250.44\n",
      "Eval num_timesteps=1650000, episode_reward=390.60 +/- 113.94\n",
      "Episode length: 2789.20 +/- 747.76\n",
      "Eval num_timesteps=1700000, episode_reward=394.80 +/- 64.25\n",
      "Episode length: 2568.80 +/- 491.96\n",
      "Eval num_timesteps=1750000, episode_reward=390.60 +/- 111.60\n",
      "Episode length: 2855.20 +/- 611.89\n",
      "Eval num_timesteps=1800000, episode_reward=373.80 +/- 30.86\n",
      "Episode length: 2459.60 +/- 109.76\n",
      "Eval num_timesteps=1850000, episode_reward=277.20 +/- 59.99\n",
      "Episode length: 2162.60 +/- 448.18\n",
      "Eval num_timesteps=1900000, episode_reward=361.20 +/- 131.08\n",
      "Episode length: 2873.20 +/- 903.90\n",
      "Eval num_timesteps=1950000, episode_reward=344.40 +/- 111.60\n",
      "Episode length: 2398.60 +/- 488.70\n",
      "Eval num_timesteps=2000000, episode_reward=382.20 +/- 131.08\n",
      "Episode length: 2648.80 +/- 647.57\n",
      "Eval num_timesteps=2050000, episode_reward=428.40 +/- 111.60\n",
      "Episode length: 3453.40 +/- 738.42\n",
      "Eval num_timesteps=2100000, episode_reward=386.40 +/- 126.42\n",
      "Episode length: 2680.60 +/- 883.53\n",
      "Eval num_timesteps=2150000, episode_reward=491.40 +/- 60.28\n",
      "Episode length: 3534.20 +/- 749.26\n",
      "Eval num_timesteps=2200000, episode_reward=508.20 +/- 126.98\n",
      "Episode length: 3810.00 +/- 768.22\n",
      "Eval num_timesteps=2250000, episode_reward=501.80 +/- 171.84\n",
      "Episode length: 3547.60 +/- 960.13\n",
      "Eval num_timesteps=2300000, episode_reward=609.00 +/- 59.40\n",
      "Episode length: 4170.00 +/- 414.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2350000, episode_reward=541.80 +/- 107.41\n",
      "Episode length: 3586.60 +/- 678.26\n",
      "Eval num_timesteps=2400000, episode_reward=529.20 +/- 97.06\n",
      "Episode length: 3906.20 +/- 1343.72\n",
      "Eval num_timesteps=2450000, episode_reward=697.00 +/- 90.16\n",
      "Episode length: 5240.60 +/- 489.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500000, episode_reward=604.80 +/- 48.62\n",
      "Episode length: 4109.40 +/- 815.46\n",
      "Eval num_timesteps=2550000, episode_reward=588.00 +/- 85.04\n",
      "Episode length: 4063.80 +/- 865.85\n",
      "Eval num_timesteps=2600000, episode_reward=663.60 +/- 63.14\n",
      "Episode length: 4644.40 +/- 473.41\n",
      "Eval num_timesteps=2650000, episode_reward=604.80 +/- 117.60\n",
      "Episode length: 4477.00 +/- 765.07\n",
      "Eval num_timesteps=2700000, episode_reward=445.20 +/- 128.36\n",
      "Episode length: 2798.80 +/- 721.57\n",
      "Eval num_timesteps=2750000, episode_reward=617.20 +/- 111.35\n",
      "Episode length: 4320.00 +/- 471.68\n",
      "Eval num_timesteps=2800000, episode_reward=575.20 +/- 146.99\n",
      "Episode length: 3796.80 +/- 1084.82\n",
      "Eval num_timesteps=2850000, episode_reward=646.40 +/- 164.34\n",
      "Episode length: 4729.00 +/- 1324.13\n",
      "Eval num_timesteps=2900000, episode_reward=590.00 +/- 98.49\n",
      "Episode length: 4598.60 +/- 1009.74\n",
      "Eval num_timesteps=2950000, episode_reward=449.40 +/- 181.43\n",
      "Episode length: 3215.40 +/- 936.98\n",
      "Eval num_timesteps=3000000, episode_reward=608.80 +/- 167.78\n",
      "Episode length: 5059.80 +/- 1085.61\n",
      "Eval num_timesteps=3050000, episode_reward=604.40 +/- 126.60\n",
      "Episode length: 3941.60 +/- 813.22\n",
      "Eval num_timesteps=3100000, episode_reward=504.00 +/- 67.72\n",
      "Episode length: 3339.80 +/- 679.87\n",
      "Eval num_timesteps=3150000, episode_reward=533.40 +/- 84.63\n",
      "Episode length: 3618.20 +/- 661.22\n",
      "Eval num_timesteps=3200000, episode_reward=550.00 +/- 158.42\n",
      "Episode length: 3614.00 +/- 981.59\n",
      "Eval num_timesteps=3250000, episode_reward=590.00 +/- 109.10\n",
      "Episode length: 4140.60 +/- 344.19\n",
      "Eval num_timesteps=3300000, episode_reward=600.60 +/- 48.98\n",
      "Episode length: 3905.60 +/- 611.59\n",
      "Eval num_timesteps=3350000, episode_reward=613.20 +/- 128.36\n",
      "Episode length: 3775.20 +/- 1016.04\n",
      "Eval num_timesteps=3400000, episode_reward=596.40 +/- 90.67\n",
      "Episode length: 4290.60 +/- 761.74\n",
      "Eval num_timesteps=3450000, episode_reward=636.20 +/- 77.19\n",
      "Episode length: 3954.60 +/- 397.17\n",
      "Eval num_timesteps=3500000, episode_reward=600.60 +/- 90.67\n",
      "Episode length: 3777.00 +/- 627.63\n",
      "Eval num_timesteps=3550000, episode_reward=677.80 +/- 84.42\n",
      "Episode length: 4888.80 +/- 518.02\n",
      "Eval num_timesteps=3600000, episode_reward=636.20 +/- 126.51\n",
      "Episode length: 4258.80 +/- 802.20\n",
      "Eval num_timesteps=3650000, episode_reward=598.20 +/- 107.14\n",
      "Episode length: 4353.00 +/- 1106.47\n",
      "Eval num_timesteps=3700000, episode_reward=604.80 +/- 169.78\n",
      "Episode length: 4027.60 +/- 1046.13\n",
      "Eval num_timesteps=3750000, episode_reward=470.40 +/- 120.71\n",
      "Episode length: 3190.20 +/- 876.37\n",
      "Eval num_timesteps=3800000, episode_reward=638.20 +/- 147.39\n",
      "Episode length: 4127.40 +/- 965.14\n",
      "Eval num_timesteps=3850000, episode_reward=634.00 +/- 76.84\n",
      "Episode length: 4340.60 +/- 791.03\n",
      "Eval num_timesteps=3900000, episode_reward=610.80 +/- 213.84\n",
      "Episode length: 3956.00 +/- 1511.64\n",
      "Eval num_timesteps=3950000, episode_reward=527.00 +/- 185.87\n",
      "Episode length: 3484.20 +/- 1269.08\n",
      "Eval num_timesteps=4000000, episode_reward=648.60 +/- 115.12\n",
      "Episode length: 4229.00 +/- 698.07\n",
      "Eval num_timesteps=4050000, episode_reward=659.40 +/- 38.95\n",
      "Episode length: 4335.00 +/- 598.28\n",
      "Eval num_timesteps=4100000, episode_reward=470.00 +/- 270.48\n",
      "Episode length: 3148.40 +/- 1570.34\n",
      "Eval num_timesteps=4150000, episode_reward=663.60 +/- 84.63\n",
      "Episode length: 4040.40 +/- 518.94\n",
      "Eval num_timesteps=4200000, episode_reward=596.00 +/- 170.19\n",
      "Episode length: 3730.60 +/- 798.10\n",
      "Eval num_timesteps=4250000, episode_reward=745.20 +/- 77.69\n",
      "Episode length: 4223.20 +/- 454.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4300000, episode_reward=816.40 +/- 234.07\n",
      "Episode length: 5736.80 +/- 1616.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4350000, episode_reward=623.20 +/- 171.84\n",
      "Episode length: 3714.20 +/- 966.79\n",
      "Eval num_timesteps=4400000, episode_reward=692.60 +/- 110.53\n",
      "Episode length: 4522.40 +/- 752.69\n",
      "Eval num_timesteps=4450000, episode_reward=893.60 +/- 163.32\n",
      "Episode length: 5298.40 +/- 1307.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500000, episode_reward=625.60 +/- 217.82\n",
      "Episode length: 3560.80 +/- 1326.11\n",
      "Eval num_timesteps=4550000, episode_reward=711.80 +/- 32.70\n",
      "Episode length: 4479.80 +/- 211.03\n",
      "Eval num_timesteps=4600000, episode_reward=705.40 +/- 51.96\n",
      "Episode length: 4446.40 +/- 526.08\n",
      "Eval num_timesteps=4650000, episode_reward=747.40 +/- 171.89\n",
      "Episode length: 4479.80 +/- 1470.17\n",
      "Eval num_timesteps=4700000, episode_reward=1001.60 +/- 618.06\n",
      "Episode length: 6775.60 +/- 4696.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4750000, episode_reward=745.20 +/- 49.36\n",
      "Episode length: 4436.00 +/- 607.35\n",
      "Eval num_timesteps=4800000, episode_reward=867.00 +/- 417.28\n",
      "Episode length: 5533.00 +/- 2633.55\n",
      "Eval num_timesteps=4850000, episode_reward=711.80 +/- 72.76\n",
      "Episode length: 4154.20 +/- 377.11\n",
      "Eval num_timesteps=4900000, episode_reward=651.00 +/- 101.15\n",
      "Episode length: 3973.80 +/- 323.79\n",
      "Eval num_timesteps=4950000, episode_reward=688.40 +/- 80.18\n",
      "Episode length: 3989.20 +/- 727.83\n",
      "Eval num_timesteps=5000000, episode_reward=686.60 +/- 164.22\n",
      "Episode length: 4295.40 +/- 1213.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x18c70480bd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.learn(total_timesteps=parameters['total_time_steps'], callback=[eval_callback]) #[checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time Elapsed 1m steps: 84m\n",
    "* Time Elapsed 1m steps tuned: 76m\n",
    "* Time Elapsed 2m steps: 140m\n",
    "* Time Elapsed 2m steps tuned: 153m\n",
    "* Time Elapsed 5m steps: 434m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_assault_5m_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:  434.2075705130895\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Elapsed: \", (time.time() - start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet and PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "class GrayToRGBWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(GrayToRGBWrapper, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        new_shape = (old_shape[0], old_shape[1], 3)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=new_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return np.repeat(obs[..., np.newaxis], 3, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNetFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=parameters['features_dim']):\n",
    "        super(CustomResNetFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        # Load pre-trained ResNet18\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        # Replace the first convolutional layer to accept single-channel images\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Remove the fully connected layer of ResNet\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        self._features_dim = features_dim\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        # Forward pass through ResNet\n",
    "        return self.resnet(observations).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class CustomTensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomTensorboardCallback, self).__init__(verbose)\n",
    "        self.writer = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log loss, reward, variance, episode length\n",
    "        if 'losses' in self.locals:\n",
    "            self.writer.add_scalar(\"Loss/Policy Loss\", self.locals[\"losses\"].policy_loss.item(), self.num_timesteps)\n",
    "            self.writer.add_scalar(\"Loss/Value Loss\", self.locals[\"losses\"].value_loss.item(), self.num_timesteps)\n",
    "        if 'ep_info_buffer' in self.locals and len(self.locals['ep_info_buffer']) > 0:\n",
    "            self.writer.add_scalar(\"Reward/Mean Reward\", np.mean([ep_info['r'] for ep_info in self.locals['ep_info_buffer']]), self.num_timesteps)\n",
    "            self.writer.add_scalar(\"Episode/Length\", np.mean([ep_info['l'] for ep_info in self.locals['ep_info_buffer']]), self.num_timesteps)\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"./resnetppo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x206d89f0d50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the environment\n",
    "env_id = 'AssaultNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)\n",
    "#env = DummyVecEnv([lambda: GrayToRGBWrapper(env)])\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomResNetFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=parameters['features_dim']),\n",
    ")\n",
    "\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=tensorboard_log_dir, policy_kwargs=policy_kwargs, verbose=parameters['verbose'])\n",
    "\n",
    "\n",
    "custom_callback = CustomTensorboardCallback()\n",
    "model.learn(total_timesteps=parameters['total_time_steps'], callback=custom_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PPO ResNet Policy Time Elapsed: 321m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_resnet_policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"ppo_resnet_policy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'AssaultNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render(mode='human')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
