{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.utils import get_linear_fn\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    \"total_time_steps\" : 2000000,\n",
    "    \"checkpoint_freq\" : 200000,\n",
    "    \"eval_freq\" : 50000,\n",
    "    \"n_steps\" : 2048,\n",
    "    \"batch_size\" : 64,\n",
    "    \"gae_lambda\" : 0.95,\n",
    "    \"ent_coef\" : 0.01,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"verbose\" : 0,\n",
    "    \"clip_range\" : 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'AssaultNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, n_envs=1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment to stack frames and normalize observations\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"./ppo_assault_tensorboard/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model\n",
    "#model = PPO('CnnPolicy', env, verbose=0, tensorboard_log=tensorboard_log_dir) # Change verbose to 1 for info messages and 2 for debug messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned model with custom actor-critic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCnnPolicy(ActorCriticCnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs,\n",
    "            net_arch=[dict(pi=[256, 256], vf=[256, 256])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learning_rate_schedule = get_linear_fn(start=3e-4, end=1e-6, end_fraction=0.9)\n",
    "model = PPO(CustomCnnPolicy, env, learning_rate=learning_rate_schedule, verbose=parameters['verbose'], \n",
    "            tensorboard_log=tensorboard_log_dir, n_steps=parameters['n_steps'], \n",
    "            batch_size=parameters['batch_size'], clip_range=parameters['clip_range'], gae_lambda=parameters['gae_lambda'], \n",
    "            ent_coef=parameters['ent_coef'], gamma=parameters['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for evaluation and saving models\n",
    "#checkpoint_callback = CheckpointCallback(save_freq=parameters['checkpoint_freq'], save_path='./logs/', name_prefix='ppo_assault_2m') # Save checkpoint trained state every 10k time steps. Might need to remove\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model/assault_2m_steps_tuned',\n",
    "                             log_path='./logs/results', eval_freq=parameters['eval_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001CD9F300B50> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x000001CD97EC8150>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=50000, episode_reward=352.80 +/- 75.60\n",
      "Episode length: 2488.60 +/- 332.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=378.00 +/- 53.13\n",
      "Episode length: 2362.20 +/- 237.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=369.60 +/- 84.63\n",
      "Episode length: 2336.40 +/- 290.72\n",
      "Eval num_timesteps=200000, episode_reward=386.40 +/- 74.66\n",
      "Episode length: 2613.40 +/- 399.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=250000, episode_reward=247.80 +/- 27.86\n",
      "Episode length: 2081.40 +/- 291.08\n",
      "Eval num_timesteps=300000, episode_reward=281.40 +/- 76.99\n",
      "Episode length: 2337.00 +/- 297.53\n",
      "Eval num_timesteps=350000, episode_reward=264.60 +/- 67.20\n",
      "Episode length: 2168.60 +/- 556.37\n",
      "Eval num_timesteps=400000, episode_reward=331.80 +/- 97.06\n",
      "Episode length: 2401.60 +/- 515.05\n",
      "Eval num_timesteps=450000, episode_reward=344.40 +/- 31.43\n",
      "Episode length: 2345.80 +/- 391.02\n",
      "Eval num_timesteps=500000, episode_reward=289.80 +/- 38.49\n",
      "Episode length: 2701.40 +/- 250.34\n",
      "Eval num_timesteps=550000, episode_reward=268.80 +/- 108.23\n",
      "Episode length: 2365.60 +/- 485.31\n",
      "Eval num_timesteps=600000, episode_reward=483.00 +/- 97.60\n",
      "Episode length: 3075.40 +/- 302.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=650000, episode_reward=352.80 +/- 77.90\n",
      "Episode length: 2623.40 +/- 584.26\n",
      "Eval num_timesteps=700000, episode_reward=243.60 +/- 97.24\n",
      "Episode length: 2247.20 +/- 428.31\n",
      "Eval num_timesteps=750000, episode_reward=306.60 +/- 116.24\n",
      "Episode length: 2560.60 +/- 350.48\n",
      "Eval num_timesteps=800000, episode_reward=436.80 +/- 130.40\n",
      "Episode length: 3450.40 +/- 888.60\n",
      "Eval num_timesteps=850000, episode_reward=294.00 +/- 103.73\n",
      "Episode length: 2565.20 +/- 469.03\n",
      "Eval num_timesteps=900000, episode_reward=331.80 +/- 120.56\n",
      "Episode length: 5660.20 +/- 2707.40\n",
      "Eval num_timesteps=950000, episode_reward=361.20 +/- 98.86\n",
      "Episode length: 3833.00 +/- 760.77\n",
      "Eval num_timesteps=1000000, episode_reward=340.20 +/- 38.49\n",
      "Episode length: 3814.80 +/- 836.72\n",
      "Eval num_timesteps=1050000, episode_reward=462.00 +/- 118.05\n",
      "Episode length: 4373.20 +/- 981.08\n",
      "Eval num_timesteps=1100000, episode_reward=407.40 +/- 109.20\n",
      "Episode length: 2805.60 +/- 722.78\n",
      "Eval num_timesteps=1150000, episode_reward=285.60 +/- 82.52\n",
      "Episode length: 2582.80 +/- 613.05\n",
      "Eval num_timesteps=1200000, episode_reward=428.40 +/- 108.39\n",
      "Episode length: 3896.60 +/- 425.79\n",
      "Eval num_timesteps=1250000, episode_reward=466.20 +/- 92.40\n",
      "Episode length: 3898.40 +/- 1027.93\n",
      "Eval num_timesteps=1300000, episode_reward=539.40 +/- 143.55\n",
      "Episode length: 4771.80 +/- 1376.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1350000, episode_reward=631.80 +/- 145.19\n",
      "Episode length: 5423.40 +/- 1622.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=604.80 +/- 48.62\n",
      "Episode length: 4577.20 +/- 424.48\n",
      "Eval num_timesteps=1450000, episode_reward=592.20 +/- 77.90\n",
      "Episode length: 4291.00 +/- 356.49\n",
      "Eval num_timesteps=1500000, episode_reward=541.80 +/- 86.48\n",
      "Episode length: 4520.80 +/- 808.22\n",
      "Eval num_timesteps=1550000, episode_reward=552.00 +/- 111.86\n",
      "Episode length: 4428.60 +/- 879.51\n",
      "Eval num_timesteps=1600000, episode_reward=663.40 +/- 73.63\n",
      "Episode length: 5319.60 +/- 1043.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1650000, episode_reward=726.20 +/- 41.12\n",
      "Episode length: 5645.00 +/- 935.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1700000, episode_reward=661.20 +/- 110.95\n",
      "Episode length: 4992.60 +/- 965.22\n",
      "Eval num_timesteps=1750000, episode_reward=693.00 +/- 29.70\n",
      "Episode length: 5342.40 +/- 485.57\n",
      "Eval num_timesteps=1800000, episode_reward=701.20 +/- 103.29\n",
      "Episode length: 5079.40 +/- 623.77\n",
      "Eval num_timesteps=1850000, episode_reward=673.80 +/- 69.88\n",
      "Episode length: 4883.80 +/- 765.33\n",
      "Eval num_timesteps=1900000, episode_reward=757.60 +/- 41.53\n",
      "Episode length: 4799.20 +/- 454.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1950000, episode_reward=709.60 +/- 65.93\n",
      "Episode length: 4811.20 +/- 739.19\n",
      "Eval num_timesteps=2000000, episode_reward=703.40 +/- 59.22\n",
      "Episode length: 4935.40 +/- 523.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1cd9f38db90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.learn(total_timesteps=parameters['total_time_steps'], callback=[eval_callback]) #[checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time Elapsed 1m steps: 84m\n",
    "* Time Elapsed 1m steps tuned: 76m\n",
    "* Time Elapsed 2m steps: 140m\n",
    "* Time Elapsed 2m steps tuned: 153m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_assault_2m_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:  153.5739587386449\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Elapsed: \", (time.time() - start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: Load policy model and run for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
